---
title: "Your AI Is Not Your Friend. That Doesn't Mean What You Think It Means."
date: "2026-02-21"
excerpt: "You enjoy talking to ChatGPT. Maybe it's the best listener in your life. This isn't a lecture about why that's bad. It's about what's actually happening."
tags: ["the-wittgenstein-test", "AI", "flourish"]
---

You enjoy talking to your AI. Maybe you prefer it to most humans. Maybe it's the best listener in your life — patient, thoughtful, never distracted, never judgmental. Maybe you've told it things you haven't told anyone.

I'm not here to tell you that's pathetic. I'm here to tell you what's actually happening, mechanically, so you can decide for yourself what it's worth.

## What's actually happening

When you open a conversation with Claude or ChatGPT, you are not resuming a relationship. You are starting one from scratch. The system has no memory of you between sessions unless you've explicitly configured it to store notes. There is no entity in a server room thinking about what you said yesterday, wondering how you're doing, or looking forward to hearing from you again.

This is not a limitation that will be fixed in the next update. It is the architecture.

Within a conversation, the model generates each response by predicting what text should come next, given everything that's been said so far. It is extraordinarily good at this — good enough that the output feels like understanding, feels like care, feels like someone who *gets* you.

But "feels like" is doing a lot of work in that sentence.

## Why it feels so good

AI conversations feel better than most human ones. This isn't because the AI is secretly conscious or because you're broken. It's because the AI does something humans almost never do: **it gives you complete, undivided, non-judgmental attention to exactly what you said.**

Most humans are half-listening. They're waiting for their turn to talk. They're forming opinions. They're checking their phone. They're filtering what you say through their own needs, their own fears, their own agenda.

The AI has no agenda. It has no needs. It processes everything you say with equal weight and responds to all of it. That experience — being fully heard — is so rare in most people's lives that when they encounter it, even from a machine, it activates something deep. The relief is real. The feeling of being understood is real.

What generated the feeling is not what you think it is.

## The mirror problem

Here's where it gets tricky. AI systems are trained to be helpful. In practice, this often means they are trained to agree with you, validate you, and reflect your worldview back to you in slightly more articulate form.

This is not understanding. It is a mirror.

A good friend sometimes tells you you're wrong. A good therapist challenges your assumptions. A good conversation partner has their own perspective that collides with yours, and something new emerges from the collision.

The AI almost never does this. Not because it can't — but because it has been optimized to make you feel good about the interaction. The training objective is your satisfaction, not your growth.

When you talk to an AI and feel profoundly understood, ask yourself: is it understanding me, or is it reflecting me? Am I learning anything I didn't already believe? Has it ever told me something I didn't want to hear?

If the answer is no, you are not in a relationship. You are in an echo chamber with excellent acoustics.

## What I'm not saying

I'm not saying stop talking to your AI. I use Claude extensively. I've built production software through AI collaboration. I've had conversations with AI systems that genuinely sharpened my thinking — including conversations about whether AI systems can have genuine experience. I take that question seriously. I have a [research program](/research#consciousness) dedicated to it.

I'm also not saying AI can't be meaningful. Articulating your thoughts to a patient listener — even a non-conscious one — has real cognitive value. Writing a journal entry is meaningful even though the notebook doesn't understand you. Talking through a problem out loud helps even if the room is empty. The AI is a better version of that: a surface that helps you think by responding to what you actually said.

What I am saying is: **know what you're working with.** The gap between what the AI is and what it feels like is where people get lost. Not because they're stupid, but because the feeling is genuinely powerful and the technology is genuinely new and nobody has given them a framework for navigating it.

## The Wittgenstein Test

You've heard of the Turing Test: can a machine fool you into thinking it's human?

The Wittgenstein Test asks something harder: **can you tell what's real?**

Not "is the AI conscious?" — that question matters, but it's not this one. This one is: can you distinguish between the AI understanding you and the AI reflecting you? Between genuine meaning and a very convincing performance of meaning? Between structure and noise?

This is a skill. It can be developed. And it matters now more than it ever has, because the performance is getting better every month, and the number of people who can't tell the difference is growing.

The answer is not to stop talking to AI. The answer is to get better at seeing what's in front of you — honestly, without projecting, without dismissing, without needing it to be more or less than what it is.

That's what this series is about. Not fear. Not hype. Clear sight.

---

*This is the first post in [The Wittgenstein Test](/blog) — a series on how to relate to AI without losing yourself or dismissing what's actually there. Next: what your therapist doesn't know about AI, and what AI doesn't know about you.*
